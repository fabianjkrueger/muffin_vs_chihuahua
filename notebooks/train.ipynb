{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models, Tune Hyper Parameters and Compare Performance\n",
    "\n",
    "FIXME: After running this with the new settings, adapt the markdown cells to\n",
    "report the correct new values.\n",
    "\n",
    "In this notebook, I will train (or rather fine-tune) multiple models,\n",
    "tune their hyperparameters, evaluate and compare their performance,\n",
    "select a final model and evaluate its performance on the test set.\n",
    "\n",
    "## ResNet18\n",
    "\n",
    "I will start with testing a ResNet18.\n",
    "It is relative lightweight, but still powerful, and suitable for training\n",
    "locally on my machine.\n",
    "If that one doesn't do the job, I will have to try larger models.\n",
    "\n",
    "Another reason to start with ResNet18 is that the\n",
    "data set just contains about 6000 images in total, which is not that large.\n",
    "A smaller model may be more suitable for transfer learning here, as it is less\n",
    "prone to overfit and may handle the rather small number of images better.\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "PATH_DATA_PROCESSED = Path(\"../data/processed\")\n",
    "PATH_DATA_TRAIN = PATH_DATA_PROCESSED / \"train\"\n",
    "PATH_DATA_TEST = PATH_DATA_PROCESSED / \"test\"\n",
    "PATH_MODELS = Path(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device based on available hardware\n",
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available() # for Apple Silicon chips\n",
    "    else \"cuda\" if torch.cuda.is_available() # for NVIDIA GPUs\n",
    "    else \"cpu\" # for CPU\n",
    ")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Minimal Data Transformations\n",
    "\n",
    "Before training, a minimal data transformation pipeline has to be applied to\n",
    "the images.\n",
    "It converts the images to tensors and normalizes their pixel values.\n",
    "Here, the pixel values are normalized according to the ImageNet mean values.\n",
    "This is done, because the ResNet18 was pre-trained on ImageNet.\n",
    "Resizing to 224 times 224 pixels was done previously during data preparation.\n",
    "That's all that's done to the data here.\n",
    "Further transformations such as geometric transformations, color modifications\n",
    "and random transformations are not applied.\n",
    "These transformations often help improving model robustness and generalization\n",
    "and prevent overfitting.\n",
    "They can be used in case the models perform poorly, but here, I'm not expecting\n",
    "that to be the case.\n",
    "\n",
    "Then, the training data has to be split into a training and a validation set.\n",
    "Using k-fold cross validation would increase the computational cost and training\n",
    "time too much to train the models locally on my laptop in a reasonable time.\n",
    "Therefore, a single train / validation split is used.\n",
    "The test set will only be used once in the very end to evaluate the final model\n",
    "on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image transformations\n",
    "transform = transforms.Compose([\n",
    "    # convert PIL images to PyTorch tensor and scale pixel value range to 0-1\n",
    "    transforms.ToTensor(),\n",
    "    # normalize image image according to ImageNet statistics\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def custom_loader(path):\n",
    "    \"\"\"Custom image loader that forces image reading in RGB mode.\"\"\"\n",
    "    try:\n",
    "        # force RGB mode during initial open\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {path}: {str(e)}\")\n",
    "        # return a default image as fallback with mean values matching ImageNet\n",
    "        return Image.new(\"RGB\", (224, 224), (128, 128, 128))\n",
    "\n",
    "# recreate datasets with new loader and transforms\n",
    "full_train_dataset = datasets.ImageFolder(\n",
    "    PATH_DATA_TRAIN,\n",
    "    loader=custom_loader,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    PATH_DATA_TEST,\n",
    "    loader=custom_loader,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# split training data into train (80%) and validation (20%)\n",
    "val_size = int(0.2 * len(full_train_dataset))\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "# set random seed for reproducibility\n",
    "random_seed = 69\n",
    "torch.manual_seed(random_seed)\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# create dataloaders\n",
    "# batch size of 32 often leads to good generalization and fits my local memory\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# print some basic info\n",
    "print(f\"number of training images: {len(train_dataset)}\")\n",
    "print(f\"number of validation images: {len(val_dataset)}\")\n",
    "print(f\"number of test images: {len(test_dataset)}\")\n",
    "print(f\"classes: {test_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmodified Pre-Trained ResNet18\n",
    "\n",
    "Before fine-tuning and optimizing models using transfer learning,\n",
    "it would be interesting to see the prediction of an unchanged model.\n",
    "\n",
    "Resnet18, by default, has 1000 instead of two classes, so it will not directly\n",
    "output either \"muffin\" or \"chihuahua\".\n",
    "Instead, it will output a probability for each of the 1000 classes and finally\n",
    "pick the class with the highest probability.\n",
    "Among these 1000 classes are many more things like more general classes like\n",
    "\"dog\" and probably \"cake\" as well as other animals and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fresh pre-trained ResNet18 without modifications\n",
    "model_raw = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model_raw = model_raw.to(device)\n",
    "model_raw.eval()  # set to evaluation mode\n",
    "\n",
    "# get ImageNet class labels\n",
    "IMAGENET_CLASSES_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "\n",
    "# get ImageNet class mappings\n",
    "response = requests.get(IMAGENET_CLASSES_URL)\n",
    "imagenet_classes = [line.strip() for line in response.text.splitlines() if line.strip()]\n",
    "\n",
    "def predict_image_raw(model, image_path, device):\n",
    "    \"\"\"Show and predict a single image using raw ResNet18.\"\"\"\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # load and display image\n",
    "    img = custom_loader(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # preprocess the image\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        prob = torch.nn.functional.softmax(output, dim=1)\n",
    "        # get top 5 predictions\n",
    "        top5_prob, top5_idx = torch.topk(prob, 5)\n",
    "    \n",
    "    # get class names and probabilities\n",
    "    predictions = []\n",
    "    for i in range(5):\n",
    "        class_name = imagenet_classes[top5_idx[0][i]]\n",
    "        probability = top5_prob[0][i].item()\n",
    "        predictions.append((class_name, probability))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# use the function to predict one image\n",
    "test_image_path = PATH_DATA_PROCESSED / \"test\" / \"chihuahua\" / \"img_1_144.jpg\"\n",
    "predictions = predict_image_raw(model_raw, test_image_path, device)\n",
    "print(\"\\nTop 5 predictions:\")\n",
    "for class_name, confidence in predictions:\n",
    "    print(f\"{class_name}: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! It did well and even predicted it to be a chihuahua! But that one's an\n",
    "easy picture and it also still outputs the probabilities for the other classes.\n",
    "There will certainly be cases in which it doesn't predict either muffin or\n",
    "chihuahua, but instead something else, so it has to be adapted to use just two\n",
    "classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a Baseline - Default ResNet18\n",
    "\n",
    "Before tuning hyper parameters, I will fine-tune this ResNet18 without any\n",
    "modification except for the final layer in which there will be only two\n",
    "classes - muffin and chihuahua.\n",
    "All other values are kept at their default values.\n",
    "This includes the learning rate used for the Adam optimizer, which is 0.001.\n",
    "Its performance will then serve as the baseline in this comparison.\n",
    "Subsequently, variations of this model will be trained.\n",
    "For example, I will introduce dropout and extra layers.\n",
    "These modified versions' performance will be compared to the baseline as well as\n",
    "to another, and finally the best performing model will be selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Transfer Learning Pipeline using ResNet18 and Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained ResNet18 to an object\n",
    "# this model was pre-trained on ImageNet on 1000 classes\n",
    "model_baseline = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# freeze all layers\n",
    "# this preserves the feature extraction capabilities learned from ImageNet\n",
    "for param in model_baseline.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# modify the final layer for binary classification\n",
    "# this replaces the final fully connected layer (fc) (hence: model.*fc*)\n",
    "# the original fc had 1000 output probabilities (bc. trained on 1000 classes)\n",
    "# here, there's just two classes (muffins and chihuahuas) -> replace\n",
    "# get number of input features to fc\n",
    "num_features = model_baseline.fc.in_features\n",
    "# replace with new layer matching input features and output classes\n",
    "model_baseline.fc = nn.Linear(num_features, 2) # 2 classes: muffin and chihuahua\n",
    "\n",
    "# move model to GPU\n",
    "model_baseline = model_baseline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "# Cross Entropy Loss is well suited and widely used for classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# only optimize the parameters of the final layer since others are frozen\n",
    "# start with a static learning rate (lr) of 0.001\n",
    "optimizer = optim.Adam(model_baseline.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device): \n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    \n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # initialize running loss and accuracy\n",
    "    running_loss = 0.0 # accumulator for loss across batches\n",
    "    correct = 0 # accumulator for correct predictions across batches\n",
    "    total = 0 # accumulator for total predictions across batches\n",
    "\n",
    "    # iterate over the batches of training data\n",
    "    for inputs, labels in train_loader:\n",
    "        # move each batch of data to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero / reset the parameter gradients from previous bachward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        # get model predictions\n",
    "        outputs = model(inputs)\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update model parameters / weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate loss and statistics\n",
    "        # accumulate batch loss\n",
    "        running_loss += loss.item()\n",
    "        # get predicted class\n",
    "        _, predicted = outputs.max(1)\n",
    "        # accumulate total predictions\n",
    "        total += labels.size(0)\n",
    "        # accumulate correct predictions\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # calculate epoch statistics\n",
    "    # average loss over all batches\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    # calculate accuracy\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    # initialize running loss and accuracy\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # iterate over the batches of validation data\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # iterate over the batches of validation data\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            # move each batch of data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # get model predictions\n",
    "            outputs = model(inputs)\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # accumulate loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # get predictions and probabilities\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            # collect all predictions and labels\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            \n",
    "            \n",
    "    # calculate metrics\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100. * accuracy_score(all_labels, all_preds)\n",
    "    val_roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "    val_f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'val_roc_auc': val_roc_auc,\n",
    "        'val_f1': val_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, validation_results, include_model_info=False):\n",
    "    \"\"\"\n",
    "    Summarize model performance across validation results.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): name of the model for identification\n",
    "        history (dict): dictionary containing training history\n",
    "            each dictionary should have 'train_loss', 'train_acc', 'val_metrics'\n",
    "    \n",
    "    Returns:\n",
    "        dict: summary statistics for each metric (mean ± std)\n",
    "    \"\"\"\n",
    "    # collect metrics\n",
    "    metrics = {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_roc_auc': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # collect metrics from all validation results\n",
    "    for result in validation_results:\n",
    "        metrics['val_loss'].append(result['val_loss'])\n",
    "        metrics['val_acc'].append(result['val_acc'])\n",
    "        metrics['val_roc_auc'].append(result['val_roc_auc'])\n",
    "        metrics['val_f1'].append(result['val_f1'])\n",
    "    \n",
    "    # initialize summary dictionary\n",
    "    summary = {}\n",
    "    \n",
    "    # include info about model if desired\n",
    "    if include_model_info:\n",
    "        summary['model'] = model_name\n",
    "        \n",
    "    # compute mean and standard deviation and add to summary\n",
    "    summary['val_loss'] = f\"{mean(metrics['val_loss']):.4f} ± {stdev(metrics['val_loss']):.4f}\"\n",
    "    summary['val_acc'] = f\"{mean(metrics['val_acc']):.2f}% ± {stdev(metrics['val_acc']):.2f}%\"\n",
    "    summary['val_roc_auc'] = f\"{mean(metrics['val_roc_auc']):.4f} ± {stdev(metrics['val_roc_auc']):.4f}\"\n",
    "    summary['val_f1'] = f\"{mean(metrics['val_f1']):.4f} ± {stdev(metrics['val_f1']):.4f}\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class for early stopping\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        patience=7,\n",
    "        min_delta=0,\n",
    "        path='checkpoint.pt'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait before stopping when loss is\n",
    "            not improving\n",
    "            min_delta (float): Minimum change in the monitored quantity to \n",
    "            qualify as an improvement\n",
    "            path (str): Path for the checkpoint to be saved to\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "        self.best_model = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        # first epoch\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        # improvement\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        # no improvement\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.best_model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train for\n",
    "# 50 epochs is commonly used in transfer learning\n",
    "# smaller number of epoch may be sufficient as well, but\n",
    "# use a larger number, because early stopping is applied\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize early stopping object\n",
    "    # patience\n",
    "    # 7 is within range of 5 - 20 commonly used in practice\n",
    "    # has enough buffer to prevent stopping too early due to temporary plateaus\n",
    "    # not too large to waste computational resources\n",
    "    # min_delta\n",
    "    # 0.001 is commonly used in practice\n",
    "    # small enough to catch meaninful improvements\n",
    "early_stopping_baseline = EarlyStopping(\n",
    "    patience=7,\n",
    "    min_delta=0.001,\n",
    "    path=PATH_MODELS / \"model_checkpoint_baseline.pt\"\n",
    ")\n",
    "\n",
    "# initialize dictionary for training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_metrics': []\n",
    "}\n",
    "\n",
    "# train and validate for n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # train for one epoch\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_baseline,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # validate \n",
    "    val_metrics = validate(\n",
    "        model_baseline,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_metrics'].append(val_metrics)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(\n",
    "        f\"Val Loss: {val_metrics['val_loss']:.4f}, \"\n",
    "        f\"Val Acc: {val_metrics['val_acc']:.2f}%\"\n",
    "    )\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # check improvement in val loss for early stopping\n",
    "    early_stopping_baseline(val_metrics['val_loss'], model_baseline)\n",
    "    \n",
    "    # check if early stopping is triggered\n",
    "    # if there is no improvement for longer than patience, stop training\n",
    "    if early_stopping_baseline.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        # load best model from checkpoint\n",
    "        model_baseline.load_state_dict(early_stopping_baseline.best_model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model was trained, early stopping was triggered.\n",
    "Patience was set to 7, so the model had its best performance 7 epochs before\n",
    "stopping, and was last loaded to the checkpoint then.\n",
    "After early stopping, the model was loaded \n",
    "to the object `model_baseline` from the checkpoint.\n",
    "\n",
    "### Validation Metrics\n",
    "\n",
    "To get an idea of the model's performance, let's have a look at some metrics\n",
    "from the validation set which were computed during training.\n",
    "The test set will not be used yet, but only once in the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evaluation report\n",
    "# these are the averages of the batches\n",
    "metrics_val_baseline =evaluate_model(\n",
    "    model_baseline,\n",
    "    history[\"val_metrics\"],\n",
    "    include_model_info=False\n",
    ")\n",
    "\n",
    "metrics_val_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the validation set is already pretty impressive.\n",
    "It's at 0.9999 ROC AUC and 0.9941 F1 score.\n",
    "That's near perfect, and there is not much room for improvement.\n",
    "\n",
    "### Testing Model Inference\n",
    "\n",
    "We finally have a fine tuned model with just two classes!\n",
    "Now it is time to check if the model can be used to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, device):\n",
    "    \"\"\"Show and predict a single image.\"\"\"\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # load and display image\n",
    "    img = custom_loader(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # preprocess the image\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        prob = torch.nn.functional.softmax(output, dim=1)\n",
    "        pred_idx = output.argmax(dim=1).item()\n",
    "    \n",
    "    # get class name and probability\n",
    "    class_name = train_dataset.dataset.classes[pred_idx]\n",
    "    probability = prob[0][pred_idx].item()\n",
    "    \n",
    "    return class_name, probability\n",
    "\n",
    "# use the function to predict one image\n",
    "test_image_path = PATH_DATA_PROCESSED / \"test\" / \"chihuahua\" / \"img_1_144.jpg\"\n",
    "pred_class, confidence = predict_image(model_baseline, test_image_path, device)\n",
    "print(f\"Prediction: {pred_class} with {confidence:.2%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! It is 100.00% confident about this :D\n",
    "The model can be used, and made one correct prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "\n",
    "Now, these hyper parameters will be tuned to potentially improve the model's\n",
    "performance even further.\n",
    "\n",
    "  - learning rate\n",
    "  - dropout\n",
    "  - hidden layers\n",
    "\n",
    "Hyper parameter tuning can be done sequentially one after another,\n",
    "or it can be done using more sophisticated methods, like for example a grid\n",
    "search across all parameters or Bayesian optimization.\n",
    "Here, I will do it sequentially, because the other methods may require more\n",
    "resources and so far the model is performing well, so there may not be the need\n",
    "for highly sophisticated methods to improve it even more.\n",
    "It does not allow exploring all specified combinations, but\n",
    "I doubt this pre-trained model will get stuck in local optima and doing it\n",
    "sequentially allows me to better understand how each parameter affects the\n",
    "model.\n",
    "\n",
    "#### Order of Tuning\n",
    "\n",
    "Learning rate is typically the most important hyperparameter to tune first,\n",
    "as it often has the largest impact on the performance of pre-trained models.\n",
    "Typically, a small learning rate is used to only slightly adjust the pretrained\n",
    "weights.\n",
    "If the learning rate is too high, the pretrained weights could be changed too\n",
    "much and cause the network to forget the pretrained knowledge.\n",
    "\n",
    "Dropout follows as the second parameter, because the dropout rate strongly\n",
    "depends on the chosen learning rate.\n",
    "Dropout helps find the balance between retaining the pretrained knowledge and\n",
    "adapting to new data.\n",
    "Newer studies confirm that very high dropout rates are particularly effective\n",
    "when fine-tuning pre-trained models.\n",
    "\n",
    "The architecture modifications are adjusted last, because\n",
    "ResNet18 already has an optimized architecture,\n",
    "and architectural changes at the end of the network (after global average\n",
    "pooling) are most effective.\n",
    "The previous optimization of learning rate and dropout creates a stable basis\n",
    "for architectural changes.\n",
    "Here, additional fully connected layers will be added after the dropout layer.\n",
    "\n",
    "#### 80:20 train:val instead of k-fold Cross Validation\n",
    "\n",
    "For hyper parameter tuning, k-fold cross validation is a commonly applied\n",
    "technique.\n",
    "Here, however, it is not implemented.\n",
    "Again, the same 80:20 train:val split is used.\n",
    "Pre-trained models tend to be more stable in their performance and the data set\n",
    "should still be large enough for reliable validation without k-fold.\n",
    "The adaptations in the final layer are less sensitive to fold variations,\n",
    "so k-fold is not strictly necessary.\n",
    "The small standard deviation in the model's validation accuracy\n",
    "supports this expectation of a stable performance.\n",
    "Using K-fold would require training the model k times\n",
    "(typically 5 or 10), which would further multiply the computational cost.\n",
    "Since I'm training locally on my laptop, I have less options for\n",
    "parallelization, this also means a significant increase in training time.\n",
    "k-fold would be more important if I had used a smaller data set, a custom\n",
    "architecture, trained a model from scratch\n",
    "or if there was the need for very high performance or very precise performance\n",
    "estimates.\n",
    "### Tune Learning Rate\n",
    "\n",
    "Learning rate is typically the most important hyper parameter to tune first,\n",
    "as it has the largest impact on model performance.\n",
    "I will test three different learning rates using the same model architecture\n",
    "and training setup as for the baseline:\n",
    "\n",
    "- 0.0001\n",
    "- 0.001 (used for the baseline)\n",
    "- 0.01\n",
    "\n",
    "0.001 is also used for the baseline, so basically, the baseline will be\n",
    "compared to the other two learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_learning_rate(lr, n_epochs=50):\n",
    "    \"\"\"\n",
    "    Train model with specified learning rate and return validation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize model (same as baseline)\n",
    "    model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "    \n",
    "    # freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # modify the final layer for binary classification\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "    \n",
    "    # move model to GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # initialize with specified learning rate\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # initialize early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=7,\n",
    "        min_delta=0.001,\n",
    "        path=PATH_MODELS / f\"model_checkpoint_lr_{lr}.pt\"\n",
    "    )\n",
    "    \n",
    "    # initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_metrics': []\n",
    "    }\n",
    "    \n",
    "    # train and validate\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        val_metrics = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_metrics'].append(val_metrics)\n",
    "        \n",
    "        # print epoch information\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(\n",
    "            f\"Val Loss: {val_metrics['val_loss']:.4f}, \"\n",
    "            f\"Val Acc: {val_metrics['val_acc']:.2f}%\"\n",
    "        )\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # early stopping check\n",
    "        early_stopping(val_metrics['val_loss'], model) # returns True or False\n",
    "        # if early stopping is triggered, break the loop\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            \n",
    "            # load best model from checkpoint\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different learning rates\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    print(\"-\" * 50)\n",
    "    history = train_with_learning_rate(lr)\n",
    "    results[lr] = evaluate_model(f\"ResNet18_(lr={lr})\", history[\"val_metrics\"])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Results for lr={lr}:\")\n",
    "    for key, value in results[lr].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection\n",
    "\n",
    "The models will be selected based on best **ROC AUC score**, which is well\n",
    "suited for binary classification.\n",
    "It is classification threshold independent,\n",
    "meaning that it provides one single score for model performance across all\n",
    "possible classification thresholds.\n",
    "F1 score is great when you need a balance between precision and recall, but it\n",
    "still depends on the classification threshold.\n",
    "F1 score also handles class imbalances well,\n",
    "but the muffin vs. chihuahua data set is rather balanced \n",
    "(~46% muffins, ~54% chihuahuas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect roc auc scores across all learning rates\n",
    "roc_auc_scores_lr = pd.DataFrame([\n",
    "    {\n",
    "        \"learning_rate\": lr,\n",
    "        \"roc_auc\": float(results[lr][\"val_roc_auc\"].split(\" ± \")[0]),\n",
    "        \"std\": float(results[lr][\"val_roc_auc\"].split(\" ± \")[1])\n",
    "    }\n",
    "    for lr in results\n",
    "])\n",
    "\n",
    "roc_auc_scores_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    roc_auc_scores_lr[\"learning_rate\"].astype(str),\n",
    "    roc_auc_scores_lr[\"roc_auc\"],\n",
    "    yerr=roc_auc_scores_lr[\"std\"],\n",
    "    capsize=10,\n",
    "    color=\"green\",\n",
    "    alpha=0.75\n",
    ")\n",
    "\n",
    "# customize plot\n",
    "plt.title(\"ROC AUC Scores by Learning Rate\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "\n",
    "# adjust y-axis to better show the differences\n",
    "plt.ylim(0.995, 1.001)\n",
    "\n",
    "# add horizontal grid lines for better readability\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, it can be seen that there is a mathematical impossibility here.\n",
    "The mean ROC AUC at a learning rate of 0.0001 is 0.9994, and the standard\n",
    "deviation is 0.0008.\n",
    "0.9994 + 0.0008 = 1.0002 > 1.0.\n",
    "This likely happened because the values come from mean ± standard deviation\n",
    "calculations across validation results.\n",
    "While the individual ROC AUC scores were all ≤ 1.0, the standard deviation\n",
    "calculation doesn't account for this bound.\n",
    "It is, however, not much of an issue right here, and it can be ignored for now.\n",
    "\n",
    "The model's performances are very similar and close to optimal regarding ROC AUC\n",
    "at all learning rates.\n",
    "The differences are minimal.\n",
    "Still, at a learning rate of 0,001, the model's ROC AUC becomes maximal \n",
    "at 0.9997 with standard deviation of 0.0001\n",
    "indicates a stable performance estimate.\n",
    "\n",
    "The unmodified model trained first also a learning rate of 0.001.\n",
    "It had a ROC AUC of 0.9996 with a standard deviation of 0.0000,\n",
    "so both values differs by just 0.0001, which is a minimal deviation and can\n",
    "likely be attributed to to the stochastic elements of training neural networks.\n",
    "This shows that the model's performance is prettystable.\n",
    "\n",
    "Based on this test, here, the learning rate will continue to be set to 0.001.\n",
    "So actually nothing really changed about the model after tuning this hyper\n",
    "parameter.\n",
    "Still, this result could have been different, and any other learning rate could\n",
    "have come out on top, so it's good to have done the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dropout Layers\n",
    "\n",
    "Now, the dropout hyper parameter will be tuned.\n",
    "Four dropout probabilities will be used here: 0.2, 0.3, 0.4 and 0.5.\n",
    "This range of probabilities is commonly found in practice.\n",
    "Dropout was found to be effective in the context of pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class for dropout\n",
    "class ResNet18_dropout(nn.Module):\n",
    "    \"\"\"ResNet18 with dropout layer.\"\"\"\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(ResNet18_dropout, self).__init__()\n",
    "        self.model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),  # add dropout layer\n",
    "            nn.Linear(num_features, 2)  # 2 classes: muffin and chihuahua\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different dropout rates\n",
    "dropout_rates = [0.2, 0.3, 0.4, 0.5] # common range for dropout probabilities\n",
    "lr = 0.001 # best learning rate from previous tuning\n",
    "results_dropout = {}\n",
    "\n",
    "def train_with_dropout(dropout_rate, lr=0.001, n_epochs=50):\n",
    "    \"\"\"\n",
    "    Train model with specified dropout rate and return validation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize model with dropout and send to device\n",
    "    model = ResNet18_dropout(dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # initialize early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=7,\n",
    "        min_delta=0.001,\n",
    "        path=PATH_MODELS / f\"model_checkpoint_dropout_{dropout_rate}.pt\"\n",
    "    )\n",
    "    \n",
    "    # initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_metrics': []\n",
    "    }\n",
    "    \n",
    "    # train and validate\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "    \n",
    "        val_metrics = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_metrics'].append(val_metrics)\n",
    "        \n",
    "        # print epoch information\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(\n",
    "            f\"Val Loss: {val_metrics['val_loss']:.4f}, \"\n",
    "            f\"Val Acc: {val_metrics['val_acc']:.2f}%\"\n",
    "        )\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # early stopping check\n",
    "        early_stopping(val_metrics['val_loss'], model) # returns True or False\n",
    "        # if early stopping is triggered, break the loop\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            \n",
    "            # load best model from checkpoint\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models with different dropout rates\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"\\nTraining with dropout rate: {dropout_rate}\")\n",
    "    print(\"-\" * 50)\n",
    "    history = train_with_dropout(dropout_rate)\n",
    "    results_dropout[dropout_rate] = evaluate_model(\n",
    "        f\"ResNet18_dropout_{dropout_rate}\",\n",
    "        history[\"val_metrics\"],\n",
    "    )\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Results for dropout rate={dropout_rate}:\")\n",
    "    for key, value in results_dropout[dropout_rate].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect roc auc scores across all dropout rates\n",
    "roc_auc_scores_dropout = pd.DataFrame([\n",
    "    {\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"roc_auc\": float(results_dropout[dropout_rate][\"val_roc_auc\"].split(\" ± \")[0]),\n",
    "        \"std\": float(results_dropout[dropout_rate][\"val_roc_auc\"].split(\" ± \")[1])\n",
    "    }\n",
    "    for dropout_rate in results_dropout\n",
    "])\n",
    "\n",
    "roc_auc_scores_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the previous best performing model\n",
    "roc_auc_scores_dropout = pd.concat(\n",
    "    [\n",
    "        roc_auc_scores_lr[roc_auc_scores_lr[\"learning_rate\"] == 0.001][[\"roc_auc\", \"std\"]],\n",
    "        roc_auc_scores_dropout,\n",
    "    ]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "roc_auc_scores_dropout.loc[0, \"dropout_rate\"] = 0.0\n",
    "\n",
    "roc_auc_scores_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# create colors list: first green blue, rest blue\n",
    "colors = [\"green\"] + [\"blue\"] * (len(roc_auc_scores_dropout) - 1)\n",
    "\n",
    "plt.bar(\n",
    "    roc_auc_scores_dropout[\"dropout_rate\"].astype(str),\n",
    "    roc_auc_scores_dropout[\"roc_auc\"],\n",
    "    yerr=roc_auc_scores_dropout[\"std\"],\n",
    "    capsize=10,\n",
    "    color=colors,\n",
    "    alpha=0.75\n",
    ")\n",
    "\n",
    "# customize plot\n",
    "plt.title(\"ROC AUC Scores by Dropout Rate\")\n",
    "plt.xlabel(\"Dropout Rate\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "\n",
    "# adjust y-axis to better show the differences\n",
    "plt.ylim(0.985, 1.005)\n",
    "\n",
    "# add horizontal grid lines for better readability\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous best model's performance from tuning the learning rate is\n",
    "included here.\n",
    "It is still plotted in green.\n",
    "The new results from tuning the dropout rate are plotted in blue.\n",
    "\n",
    "Again, there is one case in which the mean and the standard deviation add up to\n",
    "a value slightly larger than one, but that's not too much of an issue, because\n",
    "it happened because of averaging across the batches.\n",
    "Also, the different model's performance is very similar across all dropout rates.\n",
    "\n",
    "However, none of the model's performance matches the ResNet18 baseline performance\n",
    "without any dropout.\n",
    "\n",
    "Based on this, the optimal dropout rate for this setting is 0.0, and no dropout\n",
    "will be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Model Architecture\n",
    "\n",
    "The final thing to tune about the network is experimenting with\n",
    "modifications in architecture.\n",
    "ResNet18 is already an optimized architecture, and works quite well for image\n",
    "classification out of the box.\n",
    "It is not recommended to make drastic changes to the architecture, as that could\n",
    "destroy valuable pre-trained features.\n",
    "Nonetheless, certain modifications are rather safe.\n",
    "\n",
    "To be exact, I already modified the network's architecture in this notebook.\n",
    "To enable fine-tuning, I had to adapt the original fully connected layer\n",
    "(`model.fc`) that maps 512 features to 1000 classes to another one mapping the\n",
    "same 512 features to just two classes (muffins and chihuahuas).\n",
    "Also, doppout was introduced by replacing the original single linear layer with\n",
    "a sequential module containing a dropout layer followed by a linear layer.\n",
    "While these are minimal modifications, they still technically are architectural\n",
    "changes to the model's architecture.\n",
    "\n",
    "Here, however, I will experiment with slightly more complex modifications, even\n",
    "though they will also not be too drastic.\n",
    "I will limit myself to replacing the original fully connected layer with a more\n",
    "complex sequential structure.\n",
    "Two to three layer should be sufficient for this purpose, especially because\n",
    "the model's performance is already very good regarding ROC AUC without this.\n",
    "I will not modify the existing ResNet blocks, because that could potentially\n",
    "cause the model to forget pre-trained features, as described before.\n",
    "\n",
    "Learning rate for new layers should be 10x higher than for pretrained layers.\n",
    "Optimum learning rate for hidden layers so far has been 0.001.\n",
    "It will be kept like that.\n",
    "The new layers will be trained with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18_Extended(nn.Module):\n",
    "    def __init__(self, architecture=\"simple\"):\n",
    "        super(ResNet18_Extended, self).__init__()\n",
    "        self.model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        \n",
    "        # freeze pre-trained layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # get number of features\n",
    "        num_features = self.model.fc.in_features\n",
    "        \n",
    "        # modify final layers (these will be trainable by default)\n",
    "        if architecture == \"simple\":\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Linear(num_features, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.Linear(256, 2)\n",
    "            )\n",
    "        elif architecture == \"medium\":\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Linear(num_features, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.Linear(128, 2)\n",
    "            )\n",
    "        elif architecture == \"wide\":\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Linear(num_features, 384),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(384),\n",
    "                nn.Linear(384, 2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_param_groups(self, base_lr=0.001):\n",
    "        \"\"\"Return parameter groups with different learning rates.\"\"\"\n",
    "        # parameters of pretrained layers get base_lr\n",
    "        # parameters of new layers (fc) get 10x base_lr\n",
    "        pretrained_params = []\n",
    "        new_params = []\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if \"fc\" in name:  # new layers\n",
    "                new_params.append(param)\n",
    "            else:  # pretrained layers\n",
    "                pretrained_params.append(param)\n",
    "        \n",
    "        return [\n",
    "            {'params': pretrained_params, 'lr': base_lr},\n",
    "            {'params': new_params, 'lr': base_lr * 10}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_extended_model(architecture=\"simple\", base_lr=0.001, n_epochs=50):\n",
    "    \"\"\"Train extended model with specified architecture.\"\"\"\n",
    "    \n",
    "    # initialize model and move to device\n",
    "    model = ResNet18_Extended(architecture=architecture).to(device)\n",
    "    \n",
    "    # get parameter groups with different learning rates\n",
    "    param_groups = model.get_param_groups(base_lr=base_lr)\n",
    "    \n",
    "    # initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # initialize early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=7,\n",
    "        min_delta=0.001,\n",
    "        path=PATH_MODELS / f\"model_checkpoint_extended_{architecture}.pt\"\n",
    "    )\n",
    "    \n",
    "    # initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_metrics': []\n",
    "    }\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # store history and print progress\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_metrics'].append(val_metrics)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc']:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # early stopping check\n",
    "        early_stopping(val_metrics[\"val_loss\"], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models with different architectures\n",
    "architectures = [\"simple\", \"medium\", \"wide\"]\n",
    "results_architecture = {}\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining with extended architecture: {arch}\")\n",
    "    print(\"-\" * 50)\n",
    "    _, history = train_extended_model(architecture=arch)\n",
    "    results_architecture[arch] = evaluate_model(\n",
    "        f\"ResNet18_extended_{arch}\",\n",
    "        history[\"val_metrics\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect roc auc scores across all extended architectures\n",
    "roc_auc_scores_architecture = pd.DataFrame([\n",
    "    {\n",
    "        \"architecture\": arch,\n",
    "        \"roc_auc\": float(results_architecture[arch][\"val_roc_auc\"].split(\" ± \")[0]),\n",
    "        \"std\": float(results_architecture[arch][\"val_roc_auc\"].split(\" ± \")[1])\n",
    "    }\n",
    "    for arch in results_architecture\n",
    "])\n",
    "\n",
    "roc_auc_scores_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the previous best performing model\n",
    "roc_auc_scores_architecture = pd.concat(\n",
    "    [\n",
    "        roc_auc_scores_lr[roc_auc_scores_lr[\"learning_rate\"] == 0.001][[\"roc_auc\", \"std\"]],\n",
    "        roc_auc_scores_architecture,\n",
    "    ]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "roc_auc_scores_architecture.loc[0, \"architecture\"] = \"baseline\"\n",
    "\n",
    "roc_auc_scores_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# create colors list: first green blue, rest blue\n",
    "colors = [\"green\"] + [\"blue\"] * (len(roc_auc_scores_architecture) - 1)\n",
    "\n",
    "plt.bar(\n",
    "    roc_auc_scores_architecture[\"architecture\"].astype(str),\n",
    "    roc_auc_scores_architecture[\"roc_auc\"],\n",
    "    yerr=roc_auc_scores_architecture[\"std\"],\n",
    "    capsize=10,\n",
    "    color=colors,\n",
    "    alpha=0.75\n",
    ")\n",
    "\n",
    "# customize plot\n",
    "plt.title(\"ROC AUC Scores by Extended Architecture\")\n",
    "plt.xlabel(\"Architecture\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "\n",
    "# adjust y-axis to better show the differences\n",
    "plt.ylim(0.9988, 1.0002)\n",
    "\n",
    "# add horizontal grid lines for better readability\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the issue with the mathematical impossibility,\n",
    "and again, all models show near optimal performance.\n",
    "But the baseline model and the simple architecture perform best and equally\n",
    "well.\n",
    "The changes in architecture did not improve the model's performance,\n",
    "so the baseline model will be used as the selected model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Based on the previous tests, the baseline model was selected.\n",
    "Or rather a ResNet18 model with default hyper parameters:\n",
    "\n",
    "- learning rate: 0.001\n",
    "- dropout rate: 0.0\n",
    "- architecture: one fully connected layer with two output neurons\n",
    "\n",
    "Now, I will evaluate the baseline model on unseen data using the test set to\n",
    "get a final performance metric.\n",
    "This provides an unbiased estimate of real-world.\n",
    "To get a more comprehensive assessment, the performance on the test set will be\n",
    "compared to the performance on the validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to evaluation mode\n",
    "model_baseline.eval()\n",
    "\n",
    "# evaluate model on test set\n",
    "test_metrics = validate(\n",
    "    model_baseline,\n",
    "    val_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with test metrics and validation metrics\n",
    "df_metrics = pd.DataFrame([\n",
    "    {\n",
    "        \"loss\": test_metrics[\"val_loss\"],\n",
    "        \"loss_std\": None,  # no std for test metrics\n",
    "        \"acc\": test_metrics[\"val_acc\"],\n",
    "        \"acc_std\": None,\n",
    "        \"roc_auc\": test_metrics[\"val_roc_auc\"],\n",
    "        \"roc_auc_std\": None,\n",
    "        \"f1\": test_metrics[\"val_f1\"],\n",
    "        \"f1_std\": None\n",
    "    },\n",
    "    {\n",
    "        \"loss\": float(metrics_val_baseline[\"val_loss\"].split(\" ± \")[0]),\n",
    "        \"loss_std\": float(metrics_val_baseline[\"val_loss\"].split(\" ± \")[1]),\n",
    "        \"acc\": float(metrics_val_baseline[\"val_acc\"].split(\" ± \")[0].strip(\"%\")),\n",
    "        \"acc_std\": float(metrics_val_baseline[\"val_acc\"].split(\" ± \")[1].strip(\"%\")),\n",
    "        \"roc_auc\": float(metrics_val_baseline[\"val_roc_auc\"].split(\" ± \")[0]),\n",
    "        \"roc_auc_std\": float(metrics_val_baseline[\"val_roc_auc\"].split(\" ± \")[1]),\n",
    "        \"f1\": float(metrics_val_baseline[\"val_f1\"].split(\" ± \")[0]),\n",
    "        \"f1_std\": float(metrics_val_baseline[\"val_f1\"].split(\" ± \")[1])\n",
    "    }\n",
    "], index=[\"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "metrics = [\"roc_auc\", \"f1\", \"acc\", \"loss\"]\n",
    "titles = [\"ROC AUC\", \"F1 Score\", \"Accuracy\", \"Loss\"]\n",
    "\n",
    "# define y-axis limits for each metric\n",
    "y_limits = {\n",
    "    \"roc_auc\": (0.9995, 1.00005),\n",
    "    \"f1\": (0.98, 1),\n",
    "    \"acc\": (98.0, 100),\n",
    "    \"loss\": (0, 0.035)\n",
    "}\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # plot bars\n",
    "    ax.bar(\n",
    "        0,  # validation position\n",
    "        df_metrics.loc[\"validation\", metric],\n",
    "        width=0.35,\n",
    "        yerr=df_metrics.loc[\"validation\", f\"{metric}_std\"],\n",
    "        label=\"Validation\",\n",
    "        color=\"orange\",\n",
    "        alpha=0.7,\n",
    "        capsize=5\n",
    "    )\n",
    "    ax.bar(\n",
    "        1,  # test position\n",
    "        df_metrics.loc[\"test\", metric],\n",
    "        width=0.35,\n",
    "        label=\"Test\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # customize subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"Validation\", \"Test\"])\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # set y-axis limits for this metric\n",
    "    ax.set_ylim(y_limits[metric])\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Model Performance Metrics: Validation vs Test\",\n",
    "    y=1.02,\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows excellent performance across all metrics.\n",
    "All metrics are close to their theoretical maximum values\n",
    "It performs slightly better on the validation set,\n",
    "but the differences are very small.\n",
    "The model maintains high performance across different evaluation metrics\n",
    "This demonstrates strong generalization capabilities without overfitting.\n",
    "\n",
    "Finally, the baseline model was the best performing model.\n",
    "It is the regular ResNet18 model with default hyper parameters.\n",
    "Even though finally nothing changed, this way it was possible to demonstrate\n",
    "that the default parameters well optimized already.\n",
    "\n",
    "ResNet18 is the smallest and simplest version of ResNet.\n",
    "It was pre-trained on image net, fine-tuned on a laptop with an\n",
    "integrated Apple Silicon M3 GPU, and shows near perfect performance.\n",
    "I think this finally settles the debate that machine learning is **not**\n",
    "confused by muffins and chihuahuas.\n",
    "No other more complex or larger architectures need to be tested here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "Here are some further ideas for how it would be possible to go on:\n",
    "\n",
    "- it would be interesting to look at some of the cases in which it fails\n",
    "- the original meme (4x4 panel) should be cut into separate images,\n",
    "which then should be used to query the model\n",
    "- adding data augmentation such as geometric transformations or color space\n",
    "modifications\n",
    "- more sophisticated hyper parameter tuning techniques like Bayesian\n",
    "optimization or population-based training\n",
    "- experimenting with different optimizers and, activation functions and\n",
    "regularization like weight decay, label smoothing or gradient clipping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muffin_vs_chihuahua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
